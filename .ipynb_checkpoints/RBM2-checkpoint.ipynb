{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single layer Restricted Boltzmann Machine. Containing a visible- and a hidden layer. \n",
    "    The forward pass\n",
    "    \"\"\"\n",
    "    def __init__(self, n_visible, n_hidden, weight_variance=1e-2):\n",
    "        super(RBM_layer, self).__init__()\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        # Parameters\n",
    "        self.W = nn.Parameter(torch.randn(n_visible, n_hidden)) * weight_variance\n",
    "        self.v_bias = nn.Parameter(torch.randn(n_visible)) * weight_variance #NOTE: Change the biases such that they are all initialized at 0.\n",
    "        self.h_bias = nn.Parameter(torch.randn(n_hidden)) * weight_variance\n",
    "\n",
    "    def draw_from_p(self, p):\n",
    "        \"\"\"\n",
    "        NOTE: Sometimes one would like to feed binary values through the network insted of probabilities.\n",
    "        In that case we would first need to sample from p_h. Remember that p_h is the probability of 1 so we \n",
    "        can use numpy so sample the binary values with h=1 with p_h and h=0 with 1-p_h. \n",
    "        \"\"\"\n",
    "        r = np.random.rand(p.size())\n",
    "        return F.relu(torch.sign(p - r))\n",
    "\n",
    "    def v_to_h(self, v):\n",
    "        h = self.W @ v + self.v_bias\n",
    "        p_h = 1 / (1 + np.exp(-h))\n",
    "        # h = draw_from_p(p_h)\n",
    "        return p_h\n",
    "\n",
    "    def h_to_v(self, h):\n",
    "        v = self.W.T @ h + self.h_bias\n",
    "        p_v = 1 / (1 + np.exp(-v))\n",
    "        # v = draw_from_p(p_v)\n",
    "        return p_v\n",
    "\n",
    "    def forward(self, v, n_forward_passes=1):\n",
    "        # Compute hidden variables\n",
    "        p_h = self.v_to_h(v)\n",
    "\n",
    "        # Perform more back- and forth passes through layer.\n",
    "        # NOTE Needed for learning using the Contrastive Divergence \n",
    "        for _ in range(1, n_forward_passes):\n",
    "            p_v = self.h_to_v(p_h)\n",
    "            p_h = self.v_to_h(p_v)\n",
    "\n",
    "        h = self.draw_from_p(p_h)\n",
    "        return h\n",
    "\n",
    "class RBM(nn.Module):\n",
    "    \"\"\"\n",
    "    A multi-layer Restricted Boltzmann Machine.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, n_visible, n_hidden):\n",
    "        super(RBM, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.layers = nn.ModuleList([RBM_layer(n_visible // 2**i, n_hidden // 2**i) for i  in range(n_layers)])\n",
    "\n",
    "    def forward(self, v):\n",
    "        for layer in self.layers:\n",
    "            v = layer(v)\n",
    "\n",
    "        return v\n",
    "\n",
    "    def info_layers(self):\n",
    "        print(f\"Size of the layers:\", end=\" \")\n",
    "        for i in self.layers:\n",
    "            print(i.n_visible, end=', ')\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, fname):\n",
    "        self.data = self.load_dataset(fname)\n",
    "        # self.labels = self.load_labels #NOTE: We have unlabbeled data but for future reference it would be loaded here\n",
    "\n",
    "    def load_dataset(self, fname):\n",
    "        # Load the dataset stored at fname.\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of datapoints in the dataset\n",
    "        return self.data.shape[1] #NOTE: Assuming the data has shape (n_features, n_datapoints)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return the idx-th data point in the dataset\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 128\n",
      "128 64\n",
      "64 32\n",
      "32 16\n",
      "16 8\n",
      "8 4\n"
     ]
    }
   ],
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the layers: 16, 8, 4, 2, "
     ]
    }
   ],
   "source": [
    "# Initial values\n",
    "n_layers = 4\n",
    "n_visible = 16\n",
    "\n",
    "# Create model\n",
    "rbm = RBM(n_layers, n_visible, n_visible // 2)\n",
    "\n",
    "rbm.info_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Training the layers seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, num_epochs):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Training the layers together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "37b22bc5a805dff6c1721636ac9d80c3f92f630909bfe11b0de479b14a3a34e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
